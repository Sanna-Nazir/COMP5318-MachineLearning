{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "490517666_490517677_490528857.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VX-xzXcFdgB",
        "colab_type": "text"
      },
      "source": [
        "# COMP5318 - Machine Learning and Data Mining: Assignment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWPggS8QfEAA",
        "colab_type": "text"
      },
      "source": [
        "### NOTE: PLEASE GO THROUGH APPENDIX OF REPORT FOR DETAILED CODE RUNNING INSTRUCTIONS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nbpresent": {
          "id": "375753da-1c6c-4b02-986a-6e3b185a5869"
        },
        "id": "nInY4EGqFdgD",
        "colab_type": "text"
      },
      "source": [
        "# INTRODUCTION\n",
        "The goal of this assignment is to build a classifier to classify some grayscale images of the size **28x28** into **10** different mentioned classes. **PCA** is used as a pre-processing technique to reduce the amount of computation as the dimension of original data is large.<br /> **Four** different classifiers including **Neural Network, K- Nearest Neighbour, Logistic Regresssion and Naive Bayes** have been used in order toachieve the goal and draw comparison based on acurracy and computing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ark-OgPFdgE",
        "colab_type": "text"
      },
      "source": [
        "## Dataset description\n",
        "The provided dataset consists of a training set of 30,000 samples and a test set of 5,000 samples. They belong to 10 different categories/classes. The labels of the first 2,000 test examples were given, so the performance of our proposed methods are analyzed by exploiting the 2,000 labelled test samples.The rest 8,000 labels of the test set are reserved.<br /> **NONE of the samples from testing set were used for training.** <br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smlmhZrbFdgG",
        "colab_type": "text"
      },
      "source": [
        "## Class Labels for provided dataset:<br />\n",
        "0 T-shirt/Top<br />\n",
        "1 Trouser<br />\n",
        "2 Pullover<br />\n",
        "3 Dress<br />\n",
        "4 Coat<br />\n",
        "5 Sandal<br />\n",
        "6 Shirt<br />\n",
        "7 Sneaker<br />\n",
        "8 Bag<br />\n",
        "9 Ankle boot <br />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5w0gIa4FdgI",
        "colab_type": "text"
      },
      "source": [
        "### How to load the data\n",
        "There is a data folder with 4 main files (which can be downloaded from Canvas):\n",
        "\n",
        "    1. images_training.h5\n",
        "    2. labels_training.h5\n",
        "    3. images_testing.h5\n",
        "    4. labels_testing_2000.h5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iByTISFjFdgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_9dmS4yrZ2WL"
      },
      "source": [
        "Then **training data** is a numpy array of the shape (30000, 784), and **training label** is a numpy array of the shape (30000, ).<br />\n",
        "The **testing data** is a numpy array of shape (10000,784) and **testing label** is a numpy array of shape (2000, ). Thus, we have sliced aur **training data** to (2000, 784)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PdRs4fJDaBVJ"
      },
      "source": [
        "To read the hdf5 file and load the data into a numpy array, assuming the **training and testing data files are uploaded in the current folder of G-drive** . <br /> Use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEUztu-rFdgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('./images_training.h5','r') as H:\n",
        "    data_train = np.copy(H['datatrain'])\n",
        "with h5py.File('./labels_training.h5','r') as H:\n",
        "    label_train = np.copy(H['labeltrain'])\n",
        "\n",
        "with h5py.File('./images_testing.h5','r') as H:\n",
        "    data_test = np.copy(H['datatest'])\n",
        "\n",
        "with h5py.File('./labels_testing_2000.h5','r') as H:\n",
        "    label_test = np.copy(H['labeltest'])\n",
        "\n",
        "# using H['datatest'], H['labeltest'] for test dataset.\n",
        "\n",
        "m_train=30000\n",
        "X_train= data_train\n",
        "y_train= label_train\n",
        "X_test= data_test[0:2000]\n",
        "y_test= label_test\n",
        "print(data_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8cTMgBcFdgU",
        "colab_type": "text"
      },
      "source": [
        "Showing a sample data belonging to it's corresponding **Class**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCNfi3IsFdgV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "X_train = X_train.reshape((X_train.shape[0], 28, 28))\n",
        "print(X_train.shape)\n",
        "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
        "plt.title(\"class \" + str(y_train[0]) + \": T-shirt/Top\" )\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEtNkEHvakbL",
        "colab_type": "text"
      },
      "source": [
        "Executing **Hot Label Encoding** just for test labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oLpkJ5Xncp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "one_hot_labels = np.zeros((30000, 10))\n",
        "\n",
        "for i in range(30000):\n",
        "    one_hot_labels[i, label_train[i]] = 1\n",
        "\n",
        "print(one_hot_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfN3cNOla4Fp",
        "colab_type": "text"
      },
      "source": [
        "Reshaping training data back to original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoGJBZR2mTb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(m_train, -1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz0MDeX3U6vo",
        "colab_type": "text"
      },
      "source": [
        "## 'Part A' :Pre-Processing Technique used:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1fjri3kNAAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = data_train.T\n",
        "covariance_matrix = np.cov(features)\n",
        "\n",
        "eig_value, eig_vector = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "print('Total No. of eigen values ')\n",
        "print(eig_value.shape)\n",
        "print('\\n')\n",
        "\n",
        "sum(eig_value[0:350])/sum(eig_value)\n",
        "len(eig_value)\n",
        "eig_pairs = [(np.abs(eig_value[i]), eig_vector[:,i]) for i in range(len(eig_value))]\n",
        "\n",
        "#Eigenvalues in descending order\n",
        "eig_pairs.sort()\n",
        "eig_pairs.reverse()\n",
        "\n",
        "\n",
        "#Complete hstack of 350 features having more weightage:\n",
        "matrix_w=eig_pairs[0][1].reshape(784,1)\n",
        "for i in range(1,350):\n",
        "  a = eig_pairs[i][1].reshape(784,1)\n",
        "  matrix_w = np.hstack((matrix_w, a))\n",
        "  \n",
        "\n",
        "data_train_pca = data_train.dot(matrix_w)\n",
        "\n",
        "data_test_pca = data_test.dot(matrix_w)\n",
        "\n",
        "\n",
        "print('Size of new Projection matrix post PCA:')\n",
        "print(matrix_w.shape)\n",
        "print('\\n')\n",
        "print('Size of new data_train_PCA matrix:')\n",
        "print(data_train_pca.shape)\n",
        "print('\\n')\n",
        "print('Size of new data_test_PCA matrix:')\n",
        "print(data_test_pca.shape)\n",
        "\n",
        "data_train = data_train_pca\n",
        "data_test = data_test_pca\n",
        "X_train= data_train\n",
        "y_train= label_train\n",
        "X_test= data_test[0:2000]\n",
        "y_test= label_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-7Fyo6HcmAM",
        "colab_type": "text"
      },
      "source": [
        "## 'Part B' :Classifiers Used:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg2qTD9vVAwZ",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 1.Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSojrE4hiDrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    x = x - np.max(x)\n",
        "    fin = (np.exp(x).T / np.sum(np.exp(x),axis=1))\n",
        "    return fin\n",
        "    \n",
        "def calc_cost(x, y):\n",
        "    return np.squeeze(- np.sum(np.multiply(np.log(x), y))/y.shape[1])\n",
        "\n",
        "def fwd_propogation(X_train, W1, W2, b1, b2):\n",
        "    if W1.shape[1] != X_train.shape[0]:\n",
        "      X_train=X_train.T\n",
        "    Z_one = np.dot(W1, X_train) + b1\n",
        "    A_one = np.tanh(Z_one)\n",
        "    Z_two = np.dot(W2, A_one) + b2\n",
        "    A_two = softmax(Z_two.T)\n",
        "\n",
        "    return {\"Z_one\": Z_one,\n",
        "            \"A_one\": A_one,\n",
        "            \"Z_two\": Z_two,\n",
        "            \"A_two\": A_two}\n",
        "\n",
        "def back_propogation(W1, W2, zaza, X_train, y_train):\n",
        "    n = y_train.shape[1]\n",
        "    A_one = zaza['A_one']\n",
        "    A_two = zaza['A_two']\n",
        "\n",
        "    d_Z2 = A_two - y_train\n",
        "    d_W2 = (1 / n) * np.dot(d_Z2, A_one.T)\n",
        "    d_b2 = (1 / n) * np.sum(d_Z2, axis=1, keepdims=True)\n",
        "\n",
        "    d_Z1 = np.multiply(np.dot(W2.T, d_Z2), 1 - np.square(A_one))\n",
        "    d_W1 = (1 / n) * np.dot(d_Z1, X_train.T)\n",
        "    d_b1 = (1 / n) * np.sum(d_Z1, axis=1, keepdims=True)\n",
        "\n",
        "    return {\"d_W1\": d_W1,\n",
        "            \"d_b1\": d_b1,\n",
        "            \"d_W2\": d_W2,\n",
        "            \"d_b2\": d_b2}\n",
        "\n",
        "def update_parameters(W1, W2, b1, b2, grad, learning_rate):\n",
        "    W1 = W1 - learning_rate * grad['d_W1']\n",
        "    b1 = b1 - learning_rate * grad['d_b1']\n",
        "    W2 = W2 - learning_rate * grad['d_W2']\n",
        "    b2 = b2 - learning_rate * grad['d_b2']\n",
        "\n",
        "    return W1, W2, b1, b2\n",
        "\n",
        "def nn_predict(W1, W2, b1, b2, X_train):\n",
        "    zaza = fwd_propogation(X_train, W1, W2, b1, b2)\n",
        "    prediction = np.argmax(zaza['A_two'], axis=0)\n",
        "    return prediction\n",
        "\n",
        "def neuralnet(X_train, y_train,Y_real,x_test,y_test, label_test, sizeof_h, epochs, learning_rate):\n",
        "    np.random.seed(42)\n",
        "    sizeof_x=X_train.shape[0]\n",
        "    sizeof_y=y_train.shape[0]\n",
        "    \n",
        "    # layers\n",
        "    W1 = np.random.randn(sizeof_h, sizeof_x) * 0.01\n",
        "    b1 = np.random.rand(sizeof_h, 1)\n",
        "    W2 = np.random.rand(sizeof_y, sizeof_h)\n",
        "    b2 = np.random.rand(sizeof_y, 1)\n",
        "    \n",
        "\n",
        "    nn_costs = []\n",
        "    i=0\n",
        "    while i in range(epochs):\n",
        "\n",
        "        zaza = fwd_propogation(X_train, W1, W2, b1, b2)\n",
        "\n",
        "        nn_cost = calc_cost(zaza['A_two'], y_train)\n",
        "        grad = back_propogation(W1, W2, zaza, X_train, y_train)\n",
        "        if (i > 1500):\n",
        "            learning_rate1 = 0.95*learning_rate\n",
        "            W1, W2, b1, b2 = update_parameters(W1, W2, b1, b2, grad, learning_rate1)\n",
        "        else:\n",
        "            W1, W2, b1, b2 = update_parameters(W1, W2, b1, b2, grad, learning_rate)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            nn_costs.append(nn_cost)\n",
        "            print(\"Cost after %i iteration(s): %f\" % (i, nn_cost))\n",
        "\n",
        "        i = i + 1\n",
        "\n",
        "\n",
        "    prediction = nn_predict(W1, W2, b1, b2, X_train)\n",
        "    print(\"Train accuracy: {} %\", sum(prediction == Y_real) / (float(len(Y_real))) * 100)\n",
        "    prediction=nn_predict(W1, W2, b1, b2, X_test)\n",
        "    print(\"Test accuracy: {} %\", sum(prediction == y_test) / (float(len(y_test))) * 100)\n",
        "    output= nn_predict(W1, W2, b1, b2, data_test)\n",
        "\n",
        "\n",
        "    plt.plot(nn_costs)\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('epochs (x100)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSEHdUNSiHLe",
        "colab_type": "text"
      },
      "source": [
        "## 2.LOGISTIC REGRESSION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0Hu6wikiKjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculating the gradient \n",
        "def lr_softmax(Z): # TODO\n",
        "    e_Z = np.exp(Z)\n",
        "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
        "    return A\n",
        "\n",
        "def grad_softmax(X, y, W):\n",
        "    temp = lr_softmax(X.dot(W))\n",
        "    sizeof_x=X.shape[0]    \n",
        "    n = range(sizeof_x)  \n",
        "    temp[n, y] = temp[n, y] - 1           \n",
        "    return X.T.dot(temp)/sizeof_x\n",
        "\n",
        "def loss_softmax(X, y, W):\n",
        "    A = lr_softmax(X.dot(W))\n",
        "    id0 = range(X.shape[0])\n",
        "    return -np.mean(np.log(A[id0, y]))\n",
        "\n",
        "# predictions\n",
        "def pred(W, X):\n",
        "    return np.argmax(lr_softmax(X.dot(W)), axis = 1)\n",
        "\n",
        "# calculating accuracy\n",
        "def accuracy(predictions,y):\n",
        "    return sum(predictions == y)/float(len(y))\n",
        "\n",
        "# classifier function\n",
        "def classifier_logistic(X, y, W, learning_rate = 0.05, epochs = 1000, tot = 1e-5, batch_size = 50):\n",
        "    W_original = W.copy()\n",
        "    epoch = 0 \n",
        "    costs = [loss_softmax(X, y, W)]\n",
        "    tot_batches = int(np.ceil(float(X.shape[0])/batch_size))\n",
        "    while epoch in range(epochs): \n",
        "        epoch = epoch + 1 \n",
        "        ids_mix = np.random.permutation(X.shape[0]) \n",
        "        for i in range(tot_batches):\n",
        "            ids_batch = ids_mix[batch_size*i:min(batch_size*(i+1), X.shape[0])] \n",
        "            batch_X = X[ids_batch]\n",
        "            batch_y = y[ids_batch]\n",
        "            W = W - learning_rate*grad_softmax(batch_X, batch_y, W)\n",
        "        cost = loss_softmax(X, y, W)\n",
        "        costs.append(cost)\n",
        "        \n",
        "        if epoch%50 == 0:\n",
        "            print(\"Cost after %i iteration(s): %f\" % (epoch, cost))\n",
        "\n",
        "        if np.linalg.norm(W - W_original)/W.size < tot:\n",
        "            break \n",
        "        \n",
        "        W_original = W.copy()\n",
        "\n",
        "    predictions = pred(W,X_train)\n",
        "    print(\"% Accuracy of model on train set:\",accuracy(predictions,y_train)*100)\n",
        "\n",
        "    predictions = pred(W,X_test)\n",
        "    print(\"% Accuracy of model on test set:\",accuracy(predictions,y_test)*100)\n",
        "    \n",
        "    output = pred(W,data_test)\n",
        "\n",
        "    plt.plot(costs)\n",
        "    plt.xlabel('number of epoches', fontsize = 12)\n",
        "    plt.ylabel('loss', fontsize = 12)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return output \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIQK3ngGJXnt",
        "colab_type": "text"
      },
      "source": [
        "## 3.KNN CLASSIFIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9sHBXw5JcB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class knn_funct():\n",
        "    \"A KNN with L2 dist\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train_funct(self, X, y):\n",
        "        self.data_train = X\n",
        "        self.label_train = y\n",
        "\n",
        "    def compute_distance(self, X):\n",
        "        num_test = X.shape[0]\n",
        "        num_train = self.data_train.shape[0]\n",
        "\n",
        "        dot_pro = np.dot(X, self.data_train.T)\n",
        "        sum_square_test = np.square(X).sum(axis = 1)\n",
        "        sum_square_train = np.square(self.data_train).sum(axis = 1)\n",
        "        \n",
        "        distance = np.sqrt(-2 * dot_pro + sum_square_train + np.matrix(sum_square_test).T)\n",
        "\n",
        "        return(distance)\n",
        "      \n",
        "    def predict_funct(self, X, k):\n",
        "        distance = self.compute_distance(X)\n",
        "\n",
        "        num_test = distance.shape[0]\n",
        "        y_predicted = np.zeros(num_test)\n",
        "\n",
        "        for i in range(num_test):\n",
        "            k_closest_y = []\n",
        "            labels = self.label_train[np.argsort(distance[i,:])].flatten()\n",
        "            # find k nearest lables\n",
        "            k_closest_y = labels[:k]\n",
        "\n",
        "            c = Counter(k_closest_y)\n",
        "            y_predicted[i] = c.most_common(1)[0][0]\n",
        "\n",
        "        return(y_predicted)\n",
        "      \n",
        "def knn_accuracy(y_pre,y):\n",
        "    count = y_pre == y\n",
        "    accuracy = count.sum()/len(count)\n",
        "    return accuracy\n",
        "\n",
        "def knn_predict(data_test, batch_size):\n",
        "  predictions = []\n",
        "\n",
        "  for i in range(int(len(data_test)/(2*batch_size))):\n",
        "      # predicts from i * batch_size to (i+1) * batch_size\n",
        "      predts = classifier.predict_funct(data_test[i * batch_size:(i+1) * batch_size], k)\n",
        "      predictions = predictions + list(predts)\n",
        "  \n",
        "    # To predict second half of test data\n",
        "\n",
        "  for i in range(int(len(data_test)/(2*batch_size)), int(len(data_test)/batch_size)):\n",
        "      #predicts from i * batch_size to (i+1) * batch_size\n",
        "      predts = classifier.predict_funct(data_test[i * batch_size:(i+1) * batch_size], k)\n",
        "      predictions = predictions + list(predts)\n",
        "\n",
        "\n",
        "  y_new = np.asarray(predictions)\n",
        "  return y_new      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_hP9qFeOT8C",
        "colab_type": "text"
      },
      "source": [
        "## 4.NAIVE BAYES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd01RB_UOWKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Naive_Bayes(object):\n",
        "    def __init__(self, learning_rate= 0.001):\n",
        "        self.learning_rate = learning_rate \n",
        "\n",
        "    def nb_train(self, X, y):\n",
        "        sample_count = X.shape[0]\n",
        "        sep = [[x for x, p in zip(X, y) if p == n] for n in np.unique(y)]\n",
        "        self.log_prior_class = [np.log(len(i) / sample_count) for i in sep]\n",
        "        count_arr = np.array([np.array(i).sum(axis=0) for i in sep]) + self.learning_rate\n",
        "        self.log_prob_feature = np.log(count_arr / count_arr.sum(axis=1)[np.newaxis].T)\n",
        "        return self\n",
        "\n",
        "    def nb_predict(self, X): # applying log-likelihood notation\n",
        "        return [(self.log_prob_feature * a).sum(axis=1) + self.log_prior_class for a in X]\n",
        "\n",
        "    def predict_nb(self, X):\n",
        "        return np.argmax(self.nb_predict(X), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKjqdz-XiLBb",
        "colab_type": "text"
      },
      "source": [
        "## 'Part C': CHOOSING AND RUNNING MODELS:\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "*   Options 1,2,3 have almost same accuracy.\n",
        "*   KNN(**option 3**) is recommended based on computing time and a bit more accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR1DEgNi0hfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Enter either 1, 2, 3 or 4 as your choice:\")\n",
        "print(\"1. Neural Network Classifier\")\n",
        "print(\"2. Logistic Regression Classifier\")\n",
        "print(\"3. K Nearest Neighbour Classifier\")\n",
        "print(\"4. Naive Bayes Classifier\")\n",
        "print()\n",
        "user_input=int(input(\"enter choice: \"))\n",
        "tic=time.time()\n",
        "if user_input is 1:\n",
        "  Y = y_train\n",
        "  y_train = (np.arange(np.max(Y) + 1) == Y[:, None]).astype(int)\n",
        "  output=neuralnet(X_train.T, y_train.T, Y, X_test.T, y_test.T, label_test, sizeof_h=100, epochs=1500,\n",
        "                                  learning_rate=0.01)\n",
        "elif user_input is 2:\n",
        "  W = np.random.randn(X_train.shape[1], 10)\n",
        "  output = classifier_logistic(X_train, y_train, W, batch_size = 50, epochs = 1200, learning_rate = 0.05)\n",
        "\n",
        "elif user_input is 3:\n",
        "  batch_size = 1000\n",
        "  # k = 4 is chosen over K=1 due to risk of overfitting.\n",
        "  k = 4\n",
        "  classifier = knn_funct()\n",
        "  classifier.train_funct(X_train, y_train)\n",
        "  predictions=knn_predict(X_train,batch_size)\n",
        "  print(\"% Accuracy of model on train set:\",knn_accuracy(predictions,y_train)*100)\n",
        "  predictions=knn_predict(X_test,batch_size)\n",
        "  print(\"% Accuracy of model on test set:\",knn_accuracy(predictions,y_test)*100)\n",
        "  output = knn_predict(data_test,batch_size)\n",
        "\n",
        "elif user_input is 4:\n",
        "  model = Naive_Bayes()\n",
        "  model.nb_train(X_train,y_train)\n",
        "  predictions = model.predict_nb(X_train)\n",
        "  print(\"% Accuracy of model on training set:\",accuracy(predictions,y_train)*100)\n",
        "  predictions = model.predict_nb(X_test)\n",
        "  print(\"% Accuracy of model on test set:\",accuracy(predictions,y_test)*100)\n",
        "  output = model.predict_nb(data_test)\n",
        "toc=time.time()\n",
        "\n",
        "print()\n",
        "print(\"Total time taken (min): \" + str((toc-tic)/60) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wC9Fmh4FdgZ",
        "colab_type": "text"
      },
      "source": [
        "### 'Part D': OUTPUT FILE FOR PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3nUUfC5Fdga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# assume output is the predicted labels\n",
        "# (5000,) \n",
        "with h5py.File('predicted_labels.h5','w') as H:\n",
        "  H.create_dataset('output',data=output)\n",
        "\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}